{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Analyze Facebook Data Using IBM Watson and IBM Data Platform\n\nThis is a three-part notebook written in `Python_3.5` meant to show how anyone can enrich and analyze a combined dataset of unstructured and strucutured information with IBM Watson and IBM Data Platform. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n\n1.  First, we use the Watson Tone Analyzer Service to enrich the Facebook Posts by pulling out `Emotion Tones` and related `Keywords`. \n\n2.  We will prep the data for analysis and visualization The end result will be a Pandas DataFrames that will contain the results of the analysis\n\n3.  Finally, we will include services from IBM's Data Platform, including IBM's own data visualization library PixieDust, to analyze the data and visualize our results.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "<a id=\"part1\"></a>\n#  Part I - Enrich\n<a id='setup'></a> \n## 1. Setup\n<a id=\"setup1\"></a>\n### 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install --upgrade watson-developer-cloud", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "!pip install --upgrade beautifulsoup4", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "If WDC or BS4 was just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"pixie\"></a>\n### 1.2 Install PixieDust and Wordcloud Libraries\nThis notebook provides an overview of how to use the PixieDust Library to analyze and visualize various data sets. If you are new to PixieDust or would like to learn more about the library, please go to this [Introductory Notebook](https://apsportal.ibm.com/exchange/public/entry/view/5b000ed5abda694232eb5be84c3dd7c1) or visit the [PixieDust Github](https://ibm-cds-labs.github.io/pixiedust/). The `Setup` section for this notebook uses instructions from the [Intro To PixieDust](https://github.com/ibm-cds-labs/pixiedust/blob/master/notebook/Intro%20to%20PixieDust.ipynb) notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To ensure you are running the latest version of PixieDust uncomment and run the following cell. Do not run this cell if you installed PixieDust locally from source and want to continue to run PixieDust from source.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install --user --upgrade pixiedust\n!pip install wordcloud", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id=\"setup2\"></a>\n### 1.3 Import Packages and Libraries\nTo check if you have package already installed, open new cell and write: *help.('Package Name')*", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import json\nimport sys\nimport watson_developer_cloud\nfrom watson_developer_cloud import ToneAnalyzerV3, VisualRecognitionV3\nimport watson_developer_cloud.natural_language_understanding.features.v1 as features\n\nimport operator\nfrom functools import reduce\nfrom io import StringIO\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\nfrom operator import itemgetter\nfrom os.path import join, dirname\nimport pandas as pd\nimport numpy as np\nimport requests\nimport pixiedust", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "<a id='setup3'></a>\n### 1.4 Add Service Credentials From Bluemix for Watson Services\n\nTo create your own service and API keys for either NLU or Tone Analyzer go to the Watson Services on [Bluemix](https://www.ibm.com/cloud-computing/bluemix/).\n\nAfter creating a service for NLU and Tone Analyzer, replace the credentials in the section below\n\n###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n                                                            username='bb8fa0e8-0e9d-4aea-acce-db7729cd7bdd',\n                                                            password='n8yqbfFjjALL')\ntone_analyzer = ToneAnalyzerV3(version='2016-05-19',\n                               username='d69ea307-c1e9-44f0-aa56-9186fa25ca8e',\n                               password='0EYsQzauIOzh')\n\nvisual_recognition = VisualRecognitionV3('2016-05-20', api_key='444d34887cf8d312596a852d862db5ccb68cc6c6')", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "<a id='load'></a> \n## 2. Load Data\n\n### 2.1 Load data from Object Storage\nIBM\u00ae Object Storage for Bluemix\u00ae provides provides you with access to a fully provisioned Swift Object Storage account to manage your data. Object Storage uses OpenStack Identity (Keystone) for authentication and can be accessed directly by using [OpenStack Object Storage (Swift) API v3](http://developer.openstack.org/api-ref-identity-v3.html#credentials-v3). \n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "###  <span style=\"color: red\"> _User Input_</span> \n\nInsert data you want to enrich by clicking on the 1001 icon on the upper right hand of the screen. Click \"Insert to code\" under the file you want to enrich. The make sure you've clicked the cell below and then choose \"Insert Pandas DataFrame.\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "# insert pandas dataframe", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Make sure this equals the variable above.\ndf = df_data_4\ndf.head()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Put in the credentials for the file you want to enrich by clicking on the 1001 icon on the upper right hand of the screen. Click the cell below, then click \"Insert to code\" under the file you want to enrich. Choose \"Insert Credentials.\" **CHANGE THE NAME TO `credentials_1`**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#choose any name to save your file\nlocalfilename = 'OutputFile.csv'", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "<a id='prepare'></a>\n## 3. Prepare Data\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id='prepare1'></a>\n###  3.1 Data Cleansing with Python\nRenaming columns, removing noticable noise in the data, pulling out URLs and appending to a new column to run through NLU", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.rename(columns={'Post Message': 'Text'}, inplace=True)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "df = df.drop([0])\ndf.head()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "#Seperate links from posts\n\ndf_http= df[\"Text\"].str.partition(\"http\")\ndf_www = df[\"Text\"].str.partition(\"www\")\n\n#combine delimiters with actual links\ndf_http[\"Link\"] = df_http[1].map(str) + df_http[2]\ndf_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n\n#include only Link columns \ndf_http.drop(df_http.columns[0:3], axis=1, inplace = True)\ndf_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n\n#merge http and www dataframes\ndfmerge = pd.concat([df_http, df_www], axis=1)\n\n#the following steps will allow you to merge data columns from the left to the right\ndfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n\n#use fillna to fill any blanks with the Link1 column\ndfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n\n#delete Link1 (www column)\ndfmerge.drop(\"Link1\", axis=1, inplace = True)\n\n#combine Link data frame \ndf = pd.concat([dfmerge,df], axis = 1)\n\n# make sure text column is a string\n# df[\"Text\"] = str(df[\"Text\"])\n\n#strip links from Text column\n#df['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\n#df['Text'] = df['Text'].apply(lambda x: x.split('www')[0])\n\ndf.head()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id='enrich'></a> \n## 4. Enrichment Time!\n<a id='nlupost'></a>\n###  4.1 NLU for the Post Text\nBelow uses Natural Language Understanding to iterate through each post and extract the enrichment features we want to use in our future analysis.\n\nEach feature we extract will be appended to the `.csv` in a new column we determine at the end of this script. If you want to run this same script for the other columns, define `free_form_responses` to the column name, if you are using URLs, change `text=response` parameter to `url=response`, and update the new column names as you see fit. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Text']\n# define the list of enrichments to apply\n# if you are modifying this script add or remove the enrichments as needed\nf = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]#'typed-rels'\n\n# Create a list to store the enriched data\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n# Go thru every reponse and enrich the text using NLU\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(nlu.analyze(text=response, features=f)))\n        #print(enriched_json)\n\n        # get the SENTIMENT score and type\n        if 'sentiment' in enriched_json:\n            if('score' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n            else:\n                overallSentimentScore.append('0')\n\n            if('label' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n            else:\n                overallSentimentType.append('0')\n\n        # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n        if 'emotion' in enriched_json:\n            me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n            highestEmotion.append(me)\n            highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n        else:\n            highestEmotion.append(\"\")\n            highestEmotionScore.append(\"\")\n\n        #iterate and get KEYWORDS with a confidence of over 50%\n        if 'keywords' in enriched_json:\n            #print((enriched_json['keywords']))\n            tmpkw = []\n            for kw in enriched_json['keywords']:\n                if(float(kw[\"relevance\"]) >= 0.5):\n                    #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n                    tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n            #convert multiple keywords in a list to a string\n            if(len(tmpkw) > 1):\n                tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n            elif(len(tmpkw) == 0):\n                tmpkw = \"\"\n            else:\n                tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n            kywords.append(tmpkw)\n        else:\n            kywords.append(\"\")\n            \n        #iterate and get Entities with a confidence of over 30%\n        if 'entities' in enriched_json:\n            #print((enriched_json['entities']))\n            tmpent = []\n            for ent in enriched_json['entities']:\n                \n                if(float(ent[\"relevance\"]) >= 0.3):\n                    tmpent.append(ent[\"type\"])\n            #convert multiple concepts in a list to a string\n            if(len(tmpent) > 1):\n                tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n            elif(len(tmpent) == 0):\n                tmpent = \"\"\n            else:\n                tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n            entities.append(tmpent)\n        else:\n            entities.append(\"\")    \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n        entities.append(' ')\n        pass\n    \n# Create columns from the list and append to the dataframe\nif highestEmotion:\n    df['TextHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['TextHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['TextOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['TextOverallSentimentScore'] = overallSentimentScore\n\ndf['TextKeywords'] = kywords\ndf['TextEntities'] = entities\ndf.head(50)\n#df.info()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": []
        }, 
        {
            "source": "After we extract all of the Keywords and Entities from each Post, we have a column with multiple Keywords, and Entities separated by commas. For our Analysis in Part II we wanted also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#choose first of Keywords,Concepts, Entities\ndf[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])\n#df.head()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": " <a id='tonepost'></a> \n### 4.4 Tone Analyzer for Post Text", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Text']\n\n#Create a list to store the enriched data\n\nhighestEmotionTone = []\nemotionToneScore = []\n\nlanguageToneScore = []\nhighestLanguageTone = []\n\nsocialToneScore = []\nhighestSocialTone = []\n\n\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(tone_analyzer.tone(text=response)))\n        #print(enriched_json)\n        \n        if 'tone_categories' in enriched_json['document_tone']:\n            me = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestEmotionTone.append(me)\n            you = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n            emotionToneScore.append(you)\n            \n            me1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestLanguageTone.append(me1)\n            you1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n            languageToneScore.append(you1)\n            \n            me2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestSocialTone.append(me2)\n            you2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n            socialToneScore.append(you2)\n            \n            \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        emotionToneScore.append(' ')\n        highestEmotionTone.append(' ')\n        languageToneScore.append(' ')\n        highestLanguageTone.append(' ')\n        socialToneScore.append(' ')\n        highestSocialTone.append(' ')\n        pass\n    \nif highestEmotionTone:\n    df['highestEmotionTone'] = highestEmotionTone    \nif emotionToneScore:\n    df['emotionToneScore'] = emotionToneScore\n    \nif languageToneScore:\n    df['languageToneScore'] = languageToneScore\nif highestLanguageTone:\n    df['highestLanguageTone'] = highestLanguageTone\n    \nif highestSocialTone:\n    df['highestSocialTone'] = highestSocialTone    \nif socialToneScore:\n    df['socialToneScore'] = socialToneScore \n    \ndf.head(50)\n#df.info()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": " <a id='write'></a>\n## Enrichment is now COMPLETE!\n<a id='write1'></a> \nLast step is to write and save the enriched dataframe to SoftLayer's Object Storage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Since we already created the `localfilename` variable in the Setup stage and defined the necessary credentials, this snippet will work for all new files and does not need to be changed.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r',encoding=\"utf-8\")\n    my_data = f.read()\n    data_to_send = my_data.encode(\"utf-8\")\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    #print(resp1_body)\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/',  local_file_name])\n                            print(url2)\n    s_subject_token = resp1.headers['x-subject-token']\n    #print(s_subject_token)\n    #print(credentials['container'])\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = data_to_send )\n    print(resp2)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "#choose any name to save your file\ndf.to_csv('OutputFile.csv',index=False)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "<a id='write2'></a> Make sure to change the \"credential\" argument below matches the variable name of the credentials you imported in the Setup Phase.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "put_file(credentials_1,localfilename)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id=\"part2\"></a> \n# Part II - Analysis\n<a id='prepare'></a>\n## 1. Tone Analysis\n <a id='visualizations'></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Post Tone Dataframe", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Create a new dataframe for analyzing the emotional tone from each Facebook entry\n# \npost_tones = [\"highestEmotionTone\"]\n\n#Create a new dataframe with tones\ndf_post_tones = df[post_tones]\n#Aggregate the tone data for Analysis\ntones = df_post_tones\ntones = pd.DataFrame(tones.groupby('highestEmotionTone').size().reset_index(name='Posts'))\ntones.head()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "highestEmotionTone", 
                        "handlerId": "pieChart", 
                        "rendererId": "matplotlib", 
                        "mpld3": "false", 
                        "aggregation": "SUM", 
                        "valueFields": "Posts", 
                        "rowCount": "500", 
                        "chartsize": "71"
                    }
                }
            }, 
            "outputs": []
        }, 
        {
            "source": "# PixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://ibm-cds-labs.github.io/pixiedust/displayapi.html. The following cell creates a DataFrame and uses the display() API to create a bar chart:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Use the Pixiedust library to easily visualize the data\ndisplay(tones)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "highestEmotionTone", 
                        "aggregation": "SUM", 
                        "handlerId": "pieChart", 
                        "valueFields": "Posts", 
                        "rowCount": "500", 
                        "chartsize": "71"
                    }
                }
            }, 
            "outputs": []
        }, 
        {
            "source": "# More Info.\nFor more information about PixieDust check out the following:\n#### PixieDust Documentation: https://ibm-cds-labs.github.io/pixiedust/index.html\n#### PixieDust GitHub Repo: https://github.com/ibm-cds-labs/pixiedust", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }
    }, 
    "nbformat_minor": 1
}